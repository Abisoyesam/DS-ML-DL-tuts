{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7388e0a",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "<hr>\n",
    "\n",
    "It is about learning the optimal behavior in an environment to obtain maximum reward. Given a set of rewards or punishment, learn actions to take in the future.\n",
    "\n",
    "![reinforcement](./images/reinforcement.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e601399c",
   "metadata": {},
   "source": [
    "**Agent**\n",
    "- The agent perform action\n",
    "- The environment gives the agent a state\n",
    "- The environment gives a state and reward (or punishment)\n",
    "\n",
    "**N.B:** This is how robots are taught to walk.\n",
    "\n",
    "## Markov Decision Process\n",
    "- Model for decision-making, representing state, action, and their reward.\n",
    "- Set of state $S$\n",
    "- Set of action $Action (x)$\n",
    "- Transition model $P(s' |s,a)$\n",
    "- Reward function $R(s,a,s')$\n",
    "\n",
    "## Q-learning (one model)\n",
    "Method for learning a function $Q(s, a)$, estimate of the value of performing action $a$ in state $s$.\n",
    "\n",
    "- Start with $Q(s, a) = 0$ for all $s, a$\n",
    "- Update $Q$ when we take an action\n",
    "- $Q(s, a) = Q(s, a) + \\alpha($reward$ + \\gamma\\max(s', a') - Q(s, a)) = (1 - \\alpha)Q(s, a) + \\alpha($reward$ + \\gamma\\max(s', a'))$\n",
    "\n",
    "$\\alpha$ - learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6667efda",
   "metadata": {},
   "source": [
    "## $\\epsilon$-Greedy Decision Making\n",
    "**Explore vs Exploit**\n",
    "- With propability $\\epsilon$ take a random move\n",
    "- Otherwise, take action $a$ with maximum $Q(s, a)$\n",
    "\n",
    "### Simple Task\n",
    "\n",
    "![field](./images/field.PNG)\n",
    "\n",
    "- Starts at a random point\n",
    "- Move left or right\n",
    "- Avoid the red box (you lose the game)\n",
    "- Find the green box (you win the game)\n",
    "\n",
    "![field](./images/field2.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74d012c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dac90304",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Field:\n",
    "    def __init__(self):\n",
    "        # define the negative, neutral, and positive state.\n",
    "        self.states = [-1,0,0,0,0,0,0,1,0,0,0]\n",
    "        self.state = random.randrange(0,len(self.states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cf03e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field = Field()\n",
    "field.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22a666ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Field:\n",
    "    def __init__(self):\n",
    "        # define the negative, neutral, and positive state.\n",
    "        self.states = [-1,0,0,0,0,0,0,1,0,0,0]\n",
    "        self.state = random.randrange(0,len(self.states))\n",
    "        \n",
    "    def done(self):\n",
    "        if self.states[self.state] != 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c51af95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field = Field()\n",
    "field.state, field.done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78468a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Field:\n",
    "    def __init__(self):\n",
    "        # define the negative, neutral, and positive state.\n",
    "        self.states = [-1,0,0,0,0,0,0,1,0,0,0]\n",
    "        self.state = random.randrange(0,len(self.states))\n",
    "        \n",
    "    def done(self):\n",
    "        if self.states[self.state] != 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def get_possible_action(self):\n",
    "        # action 0 -> left, action 1 -> right\n",
    "        actions = [0,1]\n",
    "        if self.state == 0:\n",
    "            actions.remove(0)\n",
    "        elif self.state == len(self.states) -1:\n",
    "            actions.remove(1)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88413206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, False, [0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field = Field()\n",
    "field.state, field.done(), field.get_possible_action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cee34564",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Field:\n",
    "    def __init__(self):\n",
    "        # define the negative, neutral, and positive state.\n",
    "        self.states = [-1,0,0,0,0,0,0,1,0,0,0]\n",
    "        self.state = random.randrange(0,len(self.states))\n",
    "        \n",
    "    def done(self):\n",
    "        if self.states[self.state] != 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def get_possible_action(self):\n",
    "        # action 0 -> left, action 1 -> right\n",
    "        actions = [0,1]\n",
    "        if self.state == 0:\n",
    "            actions.remove(0)\n",
    "        elif self.state == len(self.states) -1:\n",
    "            actions.remove(1)\n",
    "        return actions\n",
    "    \n",
    "    def update_next_state(self,action):\n",
    "        if action == 0:\n",
    "            if self.state == 0:\n",
    "                return self.state, -10\n",
    "            self.state -= 1\n",
    "        if action == 1:\n",
    "            if self.state == len(self.states) - 1:\n",
    "                return self.state, -10\n",
    "            self.state += 1\n",
    "            \n",
    "        reward = self.states[self.state]\n",
    "        return self.state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "12f00bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, False, [0])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field = Field()\n",
    "field.state, field.done(),field.get_possible_action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eea8a67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, -10)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field.update_next_state(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "71eb535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "field = Field()\n",
    "q_table = np.zeros((len(field.states),2))\n",
    "\n",
    "alpha = .5 # learning rate\n",
    "epsilon = .5\n",
    "gamma = .5\n",
    "\n",
    "for _ in range(10000):\n",
    "    field = Field()\n",
    "    while not field.done():\n",
    "        actions = field.get_possible_action()\n",
    "        if random.uniform(0,1) < epsilon:\n",
    "            action = random.choice(actions)\n",
    "        else:\n",
    "            action = np.argmax(q_table[field.state])\n",
    "            \n",
    "        cur_state = field.state\n",
    "        next_state, reward = field.update_next_state(action)\n",
    "        \n",
    "        q_table[cur_state, action] = (1-alpha)*q_table[cur_state,action]+alpha*(reward + gamma*np.max(q_table[next_state]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c83adae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.      ,  0.      ],\n",
       "       [-1.      ,  0.03125 ],\n",
       "       [ 0.015625,  0.0625  ],\n",
       "       [ 0.03125 ,  0.125   ],\n",
       "       [ 0.0625  ,  0.25    ],\n",
       "       [ 0.125   ,  0.5     ],\n",
       "       [ 0.25    ,  1.      ],\n",
       "       [ 0.      ,  0.      ],\n",
       "       [ 1.      ,  0.25    ],\n",
       "       [ 0.5     ,  0.125   ],\n",
       "       [ 0.25    ,  0.      ]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
